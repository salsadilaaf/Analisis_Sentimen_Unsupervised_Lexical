{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "2fLuCw5U6FV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##import"
      ],
      "metadata": {
        "id": "4DuWrlkuJppz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "5qdptUle7E5G",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "8t-z4-L06Iq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/ShopeeMain Bersih (1).xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SFTltjgi6aTp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Ed3lquRK8kws",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(subset='Review', keep='first', inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "vmvSJOGC8u-f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(drop=True, inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iRDXMlkx3cgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Memisahkan kolom Date menjadi Tanggal dan Waktu\n",
        "df['Tanggal'] = df['Date'].dt.strftime('%d/%m/%Y')  # Format DD/MM/YYYY\n",
        "df['Waktu'] = df['Date'].dt.strftime('%H:%M:%S')    # Format HH:MM:SS\n",
        "\n",
        "# Menampilkan hasil\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YynfGB_OOlLv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(df[['Tanggal','User','Review']])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SBlTGFHPFmpf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##cleaning"
      ],
      "metadata": {
        "id": "k4kLLqsD9Ici"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk"
      ],
      "metadata": {
        "id": "SOTcTUtP9JyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_URL(tweet):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', tweet)\n",
        "\n",
        "def remove_html(tweet):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', tweet)\n",
        "\n",
        "def remove_emoji(tweet):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                                u\"\\U00002702-\\U000027B0\"\n",
        "                                u\"\\U00002702-\\U000027B0\"\n",
        "                                u\"\\U000024C2-\\U0001F251\"\n",
        "                                u\"\\U0001f926-\\U0001f937\"\n",
        "                                u\"\\U00010000-\\U0010ffff\"\n",
        "                                u\"\\u200d\"\n",
        "                                u\"\\u2640-\\u2642\"\n",
        "                                u\"\\u2600-\\u2B55\"\n",
        "                                u\"\\u23cf\"\n",
        "                                u\"\\u23e9\"\n",
        "                                u\"\\u231a\"\n",
        "                                u\"\\ufe0f\"  # dingbats\n",
        "                                u\"\\u3030\"\n",
        "                                \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', tweet)\n",
        "\n",
        "def remove_numbers(tweet):\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "    return tweet\n",
        "\n",
        "def remove_symbols(tweet):\n",
        "    # Hanya menghapus simbol, mempertahankan huruf, angka, dan spasi\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "    return tweet\n",
        "\n",
        "def remove_extra_spaces(tweet):\n",
        "    # Menghapus spasi berlebihan di antara kata-kata\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
        "    return tweet\n",
        "\n",
        "# Proses cleaning data di DataFrame\n",
        "df['cleaning'] = df['Review'].apply(lambda x: remove_URL(x))\n",
        "df['cleaning'] = df['cleaning'].apply(lambda x: remove_html(x))\n",
        "df['cleaning'] = df['cleaning'].apply(lambda x: remove_emoji(x))\n",
        "df['cleaning'] = df['cleaning'].apply(lambda x: remove_numbers(x))\n",
        "df['cleaning'] = df['cleaning'].apply(lambda x: remove_symbols(x))\n",
        "df['cleaning'] = df['cleaning'].apply(lambda x: remove_extra_spaces(x))"
      ],
      "metadata": {
        "id": "nLXtsNfK-4HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "WdkY8Nr6_-ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pemisahan berdasarkan kapital"
      ],
      "metadata": {
        "id": "xBEXTomLFMkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_capital_letters(text):\n",
        "    # Memisahkan kata yang digabung berdasarkan huruf kapital\n",
        "    tokens = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?![a-z])', text)\n",
        "    return ' '.join(tokens)  # Menggabungkan kembali token dengan spasi\n",
        "\n",
        "# Terapkan fungsi split_by_capital_letters pada kolom 'cleansing'\n",
        "df['capital_split'] = df['cleaning'].apply(split_by_capital_letters)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "3ejgzaT_FPG-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##case folding"
      ],
      "metadata": {
        "id": "fsVSGgn2DFle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def case_folding(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.lower()  # Mengubah menjadi huruf kecil\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Terapkan fungsi case_folding ke kolom 'capital_split'\n",
        "df['case_folding'] = df['capital_split'].apply(case_folding)"
      ],
      "metadata": {
        "id": "Ioz_DvK5Bk1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "DTPqfGAJC89d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##normalisasi"
      ],
      "metadata": {
        "id": "ZEF4wK3x3kIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat data\n",
        "kata_baku_df = pd.read_excel('/content/kamuskatabaku.xlsx')\n",
        "\n",
        "# Mengubah kolom menjadi list\n",
        "informal = kata_baku_df['tidak_baku'].values.tolist()\n",
        "formal = kata_baku_df['kata_baku'].values.tolist()\n",
        "\n",
        "# Fungsi formalisasi sudah didefinisikan sebelumnya\n",
        "def formalize_text(text, formal, informal):\n",
        "    text = text.split(\" \")\n",
        "    for i in range(len(text)):\n",
        "        if text[i] in informal:\n",
        "            index = informal.index(text[i])\n",
        "            text[i] = formal[index]\n",
        "    return ' '.join(map(str, text))\n",
        "\n",
        "# Mengaplikasikan fungsi pada kolom 'case_folding'\n",
        "df['formal'] = df['case_folding'].apply(lambda x: formalize_text(x, formal, informal))"
      ],
      "metadata": {
        "id": "hHL3E23W3jbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U7GPFgXuafcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenization"
      ],
      "metadata": {
        "id": "beOJahmnDIFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "df['tokenization'] = df['formal'].apply(tokenize)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "gVMkOzpkDEK2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##filtering/stopword removal"
      ],
      "metadata": {
        "id": "iCQGG_Q6GVjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Baca file stopword dari txt\n",
        "with open('stopword.txt', 'r') as file:\n",
        "    stopwords_list = file.readlines()\n",
        "\n",
        "# Membersihkan karakter newline dan membuat set stopwords\n",
        "custom_stopwords = set(word.strip() for word in stopwords_list)\n",
        "\n",
        "# Fungsi untuk menghapus stopwords\n",
        "def remove_stopwords_from_txt(tokens):\n",
        "    return [word for word in tokens if word not in custom_stopwords]\n",
        "\n",
        "# Contoh penerapan\n",
        "df['filtering'] = df['tokenization'].apply(remove_stopwords_from_txt)\n"
      ],
      "metadata": {
        "id": "6RwRLZM3dGV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "KaB3SKLhhWgt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###stopword nltk"
      ],
      "metadata": {
        "id": "n6T7wFNAdHBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('indonesian'))"
      ],
      "metadata": {
        "id": "Ac2He0mlIJz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "df['filtering'] = df['tokenization'].apply(remove_stopwords)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "3uyE3LgkIZi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##steaming"
      ],
      "metadata": {
        "id": "WtEL5u4rJfy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "pKCL-UWiJhRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    return [stemmer.stem(word) for word in text]\n",
        "\n",
        "df['Preprocessing'] = df['filtering'].apply(lambda x: ' '.join(stem_text(x)))\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "v08UTpsaKQvT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Hasil Preprocessing.csv', encoding='utf8', index=False)"
      ],
      "metadata": {
        "id": "CIMfHClhjK3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('Hasil Preprocessing.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ys-qEc_Jonyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualisasi"
      ],
      "metadata": {
        "id": "zprvWyd2_Qde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "# Combine all reviews with minimal preprocessing\n",
        "text = ' '.join(df['Preprocessing'].fillna('').astype(str))\n",
        "\n",
        "# WordCloud visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "wc = WordCloud(\n",
        "    background_color=\"white\",\n",
        "    width=800,\n",
        "    height=400,\n",
        "    colormap='viridis'\n",
        ")\n",
        "wc.generate(text)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Reviews\", pad=20, fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Frequency distribution plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "tokens = text.split()\n",
        "word_counts = Counter(tokens)\n",
        "top_words = word_counts.most_common(20)\n",
        "word, count = zip(*top_words)\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(word)))\n",
        "bars = plt.bar(word, count, color=colors)\n",
        "plt.xlabel(\"Kata-Kata Sering Muncul\", fontsize=12, fontweight='bold')\n",
        "plt.ylabel(\"Jumlah Kata\", fontsize=12, fontweight='bold')\n",
        "plt.title(\"Frekuensi Kata\", fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Add frequency numbers on top of bars\n",
        "for bar, num in zip(bars, count):\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        num + 1,\n",
        "        str(num),\n",
        "        fontsize=10,\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print top 20 words and their frequencies\n",
        "print(\"\\nTop 20 kata yang paling sering muncul:\")\n",
        "for word, freq in top_words:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "id": "-hDeeBvDQkmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised Lexical"
      ],
      "metadata": {
        "id": "2pVozv3CTvZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy Sastrawi openpyxl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JW6ALlmVUz5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Load lexicon data\n",
        "# Pastikan path file sesuai dengan lokasi penyimpanan\n",
        "lexicon_positive = pd.read_excel('/content/kamus_positif.xlsx')\n",
        "lexicon_negative = pd.read_excel('/content/kamus_negatif.xlsx')\n",
        "\n",
        "# Create dictionaries from lexicons\n",
        "lexicon_positive_dict = dict(zip(lexicon_positive.iloc[:, 0], lexicon_positive.iloc[:, 1]))\n",
        "lexicon_negative_dict = dict(zip(lexicon_negative.iloc[:, 0], lexicon_negative.iloc[:, 1]))\n",
        "\n",
        "def sentiment_analysis_lexicon_indonesia(text):\n",
        "\n",
        "    # Split text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Calculate sentiment score\n",
        "    score = sum(lexicon_positive_dict.get(word, 0) for word in words) + \\\n",
        "           sum(lexicon_negative_dict.get(word, 0) for word in words)\n",
        "\n",
        "    # Determine sentiment\n",
        "    if score > 0:\n",
        "        sentiment = 'positif'\n",
        "    elif score < 0:\n",
        "        sentiment = 'negatif'\n",
        "    else:\n",
        "        sentiment = 'netral'\n",
        "\n",
        "    return score, sentiment\n",
        "\n",
        "# Apply sentiment analysis to steaming column\n",
        "results = df['Preprocessing'].apply(sentiment_analysis_lexicon_indonesia)\n",
        "results = list(zip(*results))\n",
        "\n",
        "# Add results to dataframe\n",
        "df['Score'] = results[0]  # Sentiment score\n",
        "df['Label'] = results[1]  # Sentiment label\n",
        "\n",
        "# Display specific columns\n",
        "selected_columns = df[['Tanggal', 'User', 'Review', 'Preprocessing', 'Label']]\n",
        "print(selected_columns.head())\n"
      ],
      "metadata": {
        "id": "gPeOB91sU_YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(df[['Tanggal','User','Review','Preprocessing','Label']])\n",
        "df"
      ],
      "metadata": {
        "id": "OlCsBwUvYtYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure with two subplots\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Bar Chart\n",
        "plt.subplot(1, 2, 1)\n",
        "sentiment_counts = df['Label'].value_counts()\n",
        "colors = ['#2ecc71', '#e74c3c', '#3498db']  # green for positive, red for negative, blue for neutral\n",
        "bars = plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors)\n",
        "plt.title('Distribusi Label Sentimen', pad=20, fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Label Sentimen', fontsize=12)\n",
        "plt.ylabel('Jumlah', fontsize=12)\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}\\n({height/len(df):.1%})',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "# Pie Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(sentiment_counts.values,\n",
        "        labels=sentiment_counts.index,\n",
        "        colors=colors,\n",
        "        autopct='%1.1f%%',\n",
        "        explode=(0.05, 0.05, 0.05),  # Give some separation to all pieces\n",
        "        shadow=True,\n",
        "        startangle=90)\n",
        "plt.title('Persentase Label Sentimen', pad=20, fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print numerical summary\n",
        "print(\"\\nRingkasan Analisis Sentimen:\")\n",
        "summary_df = pd.DataFrame({\n",
        "    'Jumlah': sentiment_counts,\n",
        "    'Persentase': (sentiment_counts / len(df) * 100).round(2)\n",
        "})\n",
        "print(summary_df)"
      ],
      "metadata": {
        "id": "DdLB0r55VqDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "QHmO87gi6dow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Filter review negatif\n",
        "negative_reviews = df[df['Label'] == 'negatif'][['Tanggal', 'User', 'Review', 'Preprocessing', 'Label']]\n",
        "\n",
        "# Reset index agar nomor baris dimulai dari 1\n",
        "negative_reviews = negative_reviews.reset_index(drop=True)\n",
        "\n",
        "# Membuat tabel dengan tabulate\n",
        "print(\"\\nReview dengan sentimen negatif:\")\n",
        "print(tabulate(negative_reviews, headers='keys', tablefmt='grid', showindex=True))\n",
        "\n",
        "# Menampilkan jumlah total review negatif\n",
        "print(f\"\\nTotal review negatif: {len(negative_reviews)}\")"
      ],
      "metadata": {
        "id": "6tVoLPfr7DQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##visualisasi"
      ],
      "metadata": {
        "id": "F1V1z8sofXge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Pisahkan kata-kata pada setiap label\n",
        "def count_words_by_label(df, label):\n",
        "    # Filter dataframe berdasarkan label\n",
        "    label_df = df[df['Label'] == label]\n",
        "\n",
        "    # Gabungkan semua teks pada kolom 'Preprocessing'\n",
        "    all_text = ' '.join(label_df['Preprocessing'])\n",
        "\n",
        "    # Hitung frekuensi kata\n",
        "    word_counts = Counter(all_text.split())\n",
        "    return word_counts\n",
        "\n",
        "# Hitung frekuensi kata untuk setiap label\n",
        "positif_counts = count_words_by_label(df, 'positif')\n",
        "negatif_counts = count_words_by_label(df, 'negatif')\n",
        "netral_counts = count_words_by_label(df, 'netral')\n",
        "\n",
        "# Gabungkan frekuensi kata ke dalam dataframe\n",
        "top_n = 10  # Tampilkan hanya 10 kata teratas\n",
        "freq_df = pd.DataFrame({\n",
        "    'positif': dict(positif_counts.most_common(top_n)),\n",
        "    'negatif': dict(negatif_counts.most_common(top_n)),\n",
        "    'netral': dict(netral_counts.most_common(top_n))\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "# Plot barchart\n",
        "freq_df.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Frekuensi Kata pada Setiap Label')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.xlabel('Kata')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nzgraTYmfY7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "# Fungsi untuk menghitung frekuensi kata per label\n",
        "def get_word_frequency_by_label(df, label):\n",
        "    # Mengambil semua teks dari label tertentu\n",
        "    texts = df[df['Label'] == label]['Preprocessing']\n",
        "\n",
        "    # Menggabungkan semua kata\n",
        "    all_words = ' '.join(texts).split()\n",
        "\n",
        "    # Menghitung frekuensi\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # Mengambil 20 kata teratas\n",
        "    top_20 = pd.DataFrame(word_freq.most_common(20), columns=['Kata', 'Frekuensi'])\n",
        "\n",
        "    return top_20\n",
        "\n",
        "# Membuat visualisasi untuk setiap label\n",
        "def plot_word_frequency(df, label):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    data = get_word_frequency_by_label(df, label)\n",
        "\n",
        "    # Membuat bar chart\n",
        "    sns.barplot(x='Frekuensi', y='Kata', data=data, palette='viridis')\n",
        "\n",
        "    plt.title(f'20 Kata Terbanyak untuk Label {label.title()}')\n",
        "    plt.xlabel('Frekuensi')\n",
        "    plt.ylabel('Kata')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Menjalankan analisis untuk setiap label\n",
        "labels = ['positif', 'negatif', 'netral']\n",
        "\n",
        "for label in labels:\n",
        "    # Menampilkan data frekuensi\n",
        "    print(f\"\\nFrekuensi kata untuk label {label}:\")\n",
        "    freq_data = get_word_frequency_by_label(df, label)\n",
        "    print(freq_data)\n",
        "\n",
        "    # Menampilkan visualisasi\n",
        "    plot_word_frequency(df, label)"
      ],
      "metadata": {
        "id": "T3fegGvigl5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}